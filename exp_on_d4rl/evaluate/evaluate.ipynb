{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hs/dev_ai/codes/IRPO/exp_on_d4rl\n",
      "['/home/hs/anaconda3/envs/o2o/lib/python3.8/site-packages', '/home/hs/anaconda3/envs/o2o/lib/python3.8/site-packages', '/home/hs/anaconda3/envs/o2o/lib/python38.zip', '/home/hs/anaconda3/envs/o2o/lib/python3.8', '/home/hs/anaconda3/envs/o2o/lib/python3.8/lib-dynload', '', '/home/hs/anaconda3/envs/o2o/lib/python3.8/site-packages', '/home/hs/anaconda3/envs/o2o/lib/python3.8/site-packages/setuptools/_vendor', '/home/hs/anaconda3/envs/o2o/lib/python3.8/site-packages/pybullet_envs', '/home/hs/dev_ai/codes/IRPO/exp_on_d4rl', '/home/hs/dev_ai/codes/IRPO/exp_on_d4rl', '/home/hs/dev_ai/codes/IRPO/exp_on_d4rl', '/home/hs/dev_ai/codes/IRPO/exp_on_d4rl']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import d4rl\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from stable_baselines3.ppo import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize\n",
    "\n",
    "PROJECT_ROOT_DIR = Path().absolute().parent\n",
    "print(PROJECT_ROOT_DIR)\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT_DIR))\n",
    "print(sys.path)\n",
    "\n",
    "from utils.sb3_env_utils import make_env\n",
    "from utils.load_data import load_data, load_data_from_my_cache, scale_obs, split_data\n",
    "from rollout.load_data import load_data as load_data_from_hd5\n",
    "from utils.sb3_env_wrappers import ScaledObservationWrapper\n",
    "from models.sb3_model import PPOWithBCLoss\n",
    "from utils.sb3_evaluate_kl import evaluate_policy_with_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV = \"halfcheetah\"\n",
    "ENV = \"hopper\"\n",
    "BC_EXPERIMENT_NAME = f\"iter_3/medium_{ENV}_100epochs_loss_1_annealing\"\n",
    "RL_BC_EXPERIMENT_NAME = f\"iter_3/medium_{ENV}_1e7steps_8envs_loss_1_annealing\"\n",
    "ENV_NAME = f\"{ENV}-medium-v2\"\n",
    "GAMMA = 0.98\n",
    "# EXPERT_DATA_CACHE_DIR = \"rollout/data/iter_2/medium_halfcheetah_1e7steps_8envs_loss_1_annealing.hdf5\"\n",
    "EXPERT_DATA_CACHE_DIR = \"rollout/data/iter_2/medium_hopper_1e7steps_8envs_loss_1_annealing.hdf5\"\n",
    "# EXPERT_DATA_CACHE_DIR = \"cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from /home/hs/dev_ai/codes/IRPO/exp_on_d4rl/rollout/data/iter_2/medium_hopper_1e7steps_8envs_loss_1_annealing.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 9/9 [00:00<00:00, 14.47it/s]\n"
     ]
    }
   ],
   "source": [
    "origin_env = gym.make(ENV_NAME)\n",
    "\n",
    "if EXPERT_DATA_CACHE_DIR == \"cache\":\n",
    "    print(\"load data from d4rl cache.\")\n",
    "    obs, acts, infos = load_data(origin_env)\n",
    "else:\n",
    "    data_file: Path = PROJECT_ROOT_DIR / EXPERT_DATA_CACHE_DIR\n",
    "    print(f\"load data from {str(data_file.absolute())}\")\n",
    "    dataset = load_data_from_hd5(str(data_file.absolute()))\n",
    "    obs, acts, infos = load_data_from_my_cache(dataset)\n",
    "\n",
    "scaled_obs, scaler = scale_obs(obs)\n",
    "env = ScaledObservationWrapper(env=origin_env, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose:  0\n"
     ]
    }
   ],
   "source": [
    "bc_policy_save_dir = PROJECT_ROOT_DIR / \"checkpoints\" / \"bc\" / BC_EXPERIMENT_NAME\n",
    "bc_ppo = PPOWithBCLoss.load(str((bc_policy_save_dir / \"bc_checkpoint\").absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs/anaconda3/envs/o2o/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rl_bc_policy_save_dir = PROJECT_ROOT_DIR / \"checkpoints\" / \"rl\" / RL_BC_EXPERIMENT_NAME\n",
    "rl_bc_ppo = PPOWithBCLoss.load(str((rl_bc_policy_save_dir / \"best_model\").absolute()), env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs/dev_ai/codes/IRPO/exp_on_d4rl/utils/sb3_evaluate_kl.py:84: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3195.3837537527083,\n",
       " 14.743818061427817,\n",
       " 1.071204052931675e+23,\n",
       " 3.132203402847052)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy_with_kl(model_teacher=bc_ppo, model_student=rl_bc_ppo, sample_model=bc_ppo, env=env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs/anaconda3/envs/o2o/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/hs/dev_ai/codes/IRPO/exp_on_d4rl/utils/sb3_evaluate_kl.py:84: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3257.113592207432, 5.672459996405382, inf, 115.85964490978718)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy_with_kl(model_teacher=rl_bc_ppo, model_student=bc_ppo, sample_model=rl_bc_ppo, env=env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs/anaconda3/envs/o2o/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/hs/dev_ai/codes/IRPO/exp_on_d4rl/utils/sb3_evaluate_kl.py:84: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 3255.12 +/- 5.59\n",
      "Normalized Score: 100.65\n",
      "KL (Teacher->Student): 38.181136536691234\n",
      "Action Dist KL: 3.1414043052077294\n"
     ]
    }
   ],
   "source": [
    "def get_d4rl_normalized_score(env_name, score):\n",
    "    # Reference scores from D4RL paper/wiki\n",
    "    REF_SCORES = {\n",
    "        'halfcheetah-medium-v2': {'random': -280.18, 'expert': 12135.0},\n",
    "        'hopper-medium-v2': {'random': 20.27, 'expert': 3234.3},\n",
    "        'walker2d-medium-v2': {'random': 1.62, 'expert': 4592.3}\n",
    "    }\n",
    "    \n",
    "    if env_name not in REF_SCORES:\n",
    "        return None\n",
    "        \n",
    "    ref = REF_SCORES[env_name]\n",
    "    normalized_score = (score - ref['random']) / (ref['expert'] - ref['random']) * 100\n",
    "    return normalized_score\n",
    "\n",
    "mean_reward, std_reward, mean_kl, mean_act_dist_kl = evaluate_policy_with_kl(model_teacher=bc_ppo, model_student=rl_bc_ppo, sample_model=rl_bc_ppo, env=env, n_eval_episodes=10)\n",
    "normalized_score = get_d4rl_normalized_score(ENV_NAME, mean_reward)\n",
    "\n",
    "print(f\"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "print(f\"Normalized Score: {normalized_score:.2f}\")\n",
    "print(f\"KL (Teacher->Student): {mean_kl}\")\n",
    "print(f\"Action Dist KL: {mean_act_dist_kl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "o2o",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
